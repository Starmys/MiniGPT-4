{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengzhang/anaconda3/envs/minigpt4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/chengzhang/anaconda3/envs/minigpt4/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.dist_utils import get_rank\n",
    "from minigpt4.common.registry import registry\n",
    "from minigpt4.conversation.conversation import Chat, CONV_VISION\n",
    "\n",
    "# imports modules for registration\n",
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 34.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n",
      "Load 4 training prompts\n",
      "Prompt Example \n",
      "###Human: <Img><ImageHere></Img> Could you describe the contents of this image for me? ###Assistant: \n",
      "Load BLIP2-LLM Checkpoint: /home/chengzhang/Multimodal-Quantization/MiniGPT-4/checkpoints/prerained_minigpt4_7b.pth\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace()\n",
    "args.cfg_path = 'eval_configs/minigpt4_eval.yaml'\n",
    "args.gpu_id = 0\n",
    "args.options = None\n",
    "cfg = Config(args)\n",
    "\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "chat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_modules: dict[str, torch.nn.Linear] = {}\n",
    "\n",
    "# for i, block in enumerate(model.visual_encoder.blocks):\n",
    "#     linear_modules[f'vit/{i}-qkv-proj'] = block.attn.qkv\n",
    "#     linear_modules[f'vit/{i}-o-proj'] = block.attn.proj\n",
    "#     linear_modules[f'vit/{i}-fc1'] = block.mlp.fc1\n",
    "#     linear_modules[f'vit/{i}-fc2'] = block.mlp.fc2\n",
    "\n",
    "# for i, layer in enumerate(model.Qformer.bert.encoder.layer):\n",
    "#     linear_modules[f'q-former/{i}-self-q-proj'] = layer.attention.self.query\n",
    "#     linear_modules[f'q-former/{i}-self-k-proj'] = layer.attention.self.key\n",
    "#     linear_modules[f'q-former/{i}-self-v-proj'] = layer.attention.self.value\n",
    "#     linear_modules[f'q-former/{i}-self-o-proj'] = layer.attention.output.dense\n",
    "#     if hasattr(layer, 'crossattention'):\n",
    "#         linear_modules[f'q-former/{i}-cross-q-proj'] = layer.crossattention.self.query\n",
    "#         linear_modules[f'q-former/{i}-cross-k-proj'] = layer.crossattention.self.key\n",
    "#         linear_modules[f'q-former/{i}-cross-v-proj'] = layer.crossattention.self.value\n",
    "#         linear_modules[f'q-former/{i}-cross-o-proj'] = layer.crossattention.output.dense\n",
    "#     linear_modules[f'q-former/{i}-fc1'] = layer.intermediate_query.dense\n",
    "#     linear_modules[f'q-former/{i}-fc2'] = layer.output_query.dense\n",
    "\n",
    "for i, layer in enumerate(model.llama_model.model.layers):\n",
    "    linear_modules[f'llama-ori/{i}-q-proj'] = layer.self_attn.q_proj\n",
    "    linear_modules[f'llama-ori/{i}-k-proj'] = layer.self_attn.k_proj\n",
    "    linear_modules[f'llama-ori/{i}-v-proj'] = layer.self_attn.v_proj\n",
    "    linear_modules[f'llama-ori/{i}-o-proj'] = layer.self_attn.o_proj\n",
    "    linear_modules[f'llama-ori/{i}-gate-proj'] = layer.mlp.gate_proj\n",
    "    linear_modules[f'llama-ori/{i}-down-proj'] = layer.mlp.down_proj\n",
    "    linear_modules[f'llama-ori/{i}-up-proj'] = layer.mlp.up_proj\n",
    "\n",
    "for name, module in linear_modules.items():\n",
    "    module.unique_name = name\n",
    "\n",
    "ln_modules: dict[str, torch.nn.Linear] = {}\n",
    "\n",
    "for i, layer in enumerate(model.llama_model.model.layers):\n",
    "    ln_modules[f'llama-ori/{i}-input-ln'] = layer.input_layernorm\n",
    "    ln_modules[f'llama-ori/{i}-post-attn-ln'] = layer.post_attention_layernorm\n",
    "\n",
    "for name, module in ln_modules.items():\n",
    "    module.unique_name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights():\n",
    "    for name, module in linear_modules.items():\n",
    "        torch.save(\n",
    "            module.weight,\n",
    "            f'/home/chengzhang/Multimodal-Quantization/MiniGPT-4/snapshot/weights/{name}.pt',\n",
    "        )\n",
    "\n",
    "# save_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/OK-VQA/question/OpenEnded_mscoco_val2014_questions.json') as f:\n",
    "    questions = json.loads(f.read())['questions']\n",
    "\n",
    "hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_activations(hooks):\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        q = questions[i]\n",
    "        question = q['question']\n",
    "        image_id = q['image_id']\n",
    "        question_id = q['question_id']\n",
    "        image = Image.open(f'../datasets/OK-VQA/image/val2014/COCO_val2014_{str(image_id).zfill(12)}.jpg')\n",
    "        image = chat.vis_processor(image).unsqueeze(0).to(torch.float16).to('cuda')\n",
    "\n",
    "        act_folder = f'/home/chengzhang/Multimodal-Quantization/MiniGPT-4/snapshot/activations/{question_id}'\n",
    "        os.makedirs(f'{act_folder}/vit', exist_ok=True)\n",
    "        os.makedirs(f'{act_folder}/q-former', exist_ok=True)\n",
    "        os.makedirs(f'{act_folder}/llama-ori', exist_ok=True)\n",
    "\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        hooks = []\n",
    "\n",
    "        def hook(m, input, output):\n",
    "            torch.save(input, f'{act_folder}/{m.unique_name}.pt')\n",
    "\n",
    "        for name, module in linear_modules.items():\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "        for name, module in ln_modules.items():\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "        model({'image': image, 'text_input': [question]})\n",
    "\n",
    "save_activations(hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text_activations(hooks):\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        q = questions[i]\n",
    "        question = q['question']\n",
    "        image_id = q['image_id']\n",
    "        question_id = q['question_id']\n",
    "\n",
    "        act_folder = f'/home/chengzhang/Multimodal-Quantization/MiniGPT-4/snapshot/text-activations/{question_id}'\n",
    "        os.makedirs(f'{act_folder}/llama-ori', exist_ok=True)\n",
    "\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        hooks = []\n",
    "\n",
    "        def hook(m, input, output):\n",
    "            torch.save(input, f'{act_folder}/{m.unique_name}.pt')\n",
    "\n",
    "        for name, module in linear_modules.items():\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "        for name, module in ln_modules.items():\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "        model.text_forward({'text_input': [question]})\n",
    "\n",
    "save_text_activations(hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
