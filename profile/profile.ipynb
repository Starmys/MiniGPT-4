{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengzhang/anaconda3/envs/minigpt4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/chengzhang/anaconda3/envs/minigpt4/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.dist_utils import get_rank\n",
    "from minigpt4.common.registry import registry\n",
    "from minigpt4.conversation.conversation import Chat, CONV_VISION\n",
    "\n",
    "# imports modules for registration\n",
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *\n",
    "\n",
    "from deepspeed.profiling.flops_profiler import get_model_profile\n",
    "from deepspeed.accelerator import get_accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_image(chat: Chat, question: str, image_id: int):\n",
    "    chat_state = CONV_VISION.copy()\n",
    "    img_list = []\n",
    "    img = Image.open(f'../datasets/OK-VQA/image/val2014/COCO_val2014_{str(image_id).zfill(12)}.jpg')\n",
    "    llm_message = chat.upload_img(img, chat_state, img_list)\n",
    "    chat.ask(question, chat_state)\n",
    "    llm_message = chat.answer(\n",
    "        conv=chat_state,\n",
    "        img_list=img_list,\n",
    "        num_beams=1,\n",
    "        temperature=1,\n",
    "        max_new_tokens=300,\n",
    "        max_length=2000\n",
    "    )[0]\n",
    "    return llm_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n",
      "Load 4 training prompts\n",
      "Prompt Example \n",
      "###Human: <Img><ImageHere></Img> Could you describe the contents of this image for me? ###Assistant: \n",
      "Load BLIP2-LLM Checkpoint: /home/chengzhang/Multimodal-Quantization/MiniGPT-4/checkpoints/prerained_minigpt4_7b.pth\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace()\n",
    "args.cfg_path = 'eval_configs/minigpt4_eval.yaml'\n",
    "args.gpu_id = 0\n",
    "args.options = None\n",
    "cfg = Config(args)\n",
    "\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "chat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_param_size(param: torch.nn.Parameter):\n",
    "    return param.nelement() * param.element_size()\n",
    "\n",
    "def calc_mem_size(module: torch.nn.Module):\n",
    "    return sum([calc_param_size(param) for param in module.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual_encoder: 1881.765869140625 MB\n",
      "ln_vision: 0.0107421875 MB\n",
      "Qformer: 401.068359375 MB\n",
      "query_tokens: 0.09375 MB\n",
      "llama_model: 25705.046875 MB\n"
     ]
    }
   ],
   "source": [
    "print('visual_encoder:', calc_mem_size(model.visual_encoder) / 1024 / 1024, 'MB')\n",
    "print('ln_vision:', calc_mem_size(model.ln_vision) / 1024 / 1024, 'MB')\n",
    "print('Qformer:', calc_mem_size(model.Qformer) / 1024 / 1024, 'MB')\n",
    "print('query_tokens:', calc_param_size(model.query_tokens) / 1024 / 1024, 'MB')\n",
    "# print('llama_tokenizer:', calc_mem_size(model.llama_tokenizer) / 1024 / 1024, 'MB')\n",
    "print('llama_model:', calc_mem_size(model.llama_model) / 1024 / 1024, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/OK-VQA/question/OpenEnded_mscoco_val2014_questions.json') as f:\n",
    "    questions = json.loads(f.read())['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per gpu:                                               7832.63 M\n",
      "params of model = params per GPU * mp_size:                   7832.63 M\n",
      "fwd MACs per GPU:                                             1406.86 GMACs\n",
      "fwd flops per GPU:                                            2814.2 G\n",
      "fwd flops of model = fwd flops per GPU * mp_size:             2814.2 G\n",
      "fwd latency:                                                  255.06 ms\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          11.03 TFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'MiniGPT4': '7832.63 M'}\n",
      "    MACs        - {'MiniGPT4': '1406.86 GMACs'}\n",
      "    fwd latency - {'MiniGPT4': '255.06 ms'}\n",
      "depth 1:\n",
      "    params      - {'LlamaForCausalLM': '6738.42 M'}\n",
      "    MACs        - {'LlamaForCausalLM': '887.7 GMACs'}\n",
      "    fwd latency - {'LlamaForCausalLM': '160.29 ms'}\n",
      "depth 2:\n",
      "    params      - {'LlamaModel': '6607.35 M'}\n",
      "    MACs        - {'LlamaModel': '870.14 GMACs'}\n",
      "    fwd latency - {'LlamaModel': '158.32 ms'}\n",
      "depth 3:\n",
      "    params      - {'ModuleList': '6476.27 M'}\n",
      "    MACs        - {'ModuleList': '870.14 GMACs'}\n",
      "    fwd latency - {'ModuleList': '156.65 ms'}\n",
      "depth 4:\n",
      "    params      - {'LlamaDecoderLayer': '6476.27 M'}\n",
      "    MACs        - {'LlamaDecoderLayer': '870.14 GMACs'}\n",
      "    fwd latency - {'LlamaDecoderLayer': '156.65 ms'}\n",
      "depth 5:\n",
      "    params      - {'LlamaMLP': '4328.52 M'}\n",
      "    MACs        - {'LlamaMLP': '580.02 GMACs'}\n",
      "    fwd latency - {'LlamaAttention': '82.42 ms'}\n",
      "depth 6:\n",
      "    params      - {'Linear': '6476.01 M'}\n",
      "    MACs        - {'Linear': '867.78 GMACs'}\n",
      "    fwd latency - {'Linear': '92.4 ms'}\n",
      "depth 7:\n",
      "    params      - {'Linear': '56.67 M'}\n",
      "    MACs        - {'BertSelfAttention': '8.44 GMACs'}\n",
      "    fwd latency - {'BertSelfAttention': '14.4 ms'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "MiniGPT4(\n",
      "  7832.63 M, 100.00% Params, 1406.86 GMACs, 100.00% MACs, 255.06 ms, 100.00% latency, 11.03 TFLOPS, \n",
      "  (visual_encoder): VisionTransformer(\n",
      "    985.89 M, 12.59% Params, 506.21 GMACs, 35.98% MACs, 57.12 ms, 22.40% latency, 17.73 TFLOPS, \n",
      "    (patch_embed): PatchEmbed(\n",
      "      829.31 k, 0.01% Params, 423.89 MMACs, 0.03% MACs, 260.59 us, 0.10% latency, 3.26 TFLOPS, \n",
      "      (proj): Conv2d(829.31 k, 0.01% Params, 423.89 MMACs, 0.03% MACs, 184.3 us, 0.07% latency, 4.6 TFLOPS, 3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "    )\n",
      "    (pos_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 2.17 ms, 0.85% latency, 11.94 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 70.81 us, 0.03% latency, 51.1 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 1.32 ms, 0.52% latency, 6.16 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 116.83 us, 0.05% latency, 52.33 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 293.97 us, 0.12% latency, 6.93 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.09 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 59.84 us, 0.02% latency, 60.47 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 436.78 us, 0.17% latency, 40.73 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 117.06 us, 0.05% latency, 75.97 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 50.31 us, 0.02% latency, 62.78 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 95.37 us, 0.04% latency, 93.25 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.99 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.49 ms, 0.58% latency, 17.45 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 78.2 us, 0.03% latency, 46.27 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 676.63 us, 0.27% latency, 12.05 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 106.33 us, 0.04% latency, 57.5 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 86.55 us, 0.03% latency, 23.55 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.75 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.55 us, 0.02% latency, 65.14 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 416.52 us, 0.16% latency, 42.71 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 111.82 us, 0.04% latency, 79.53 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 45.3 us, 0.02% latency, 69.71 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 93.94 us, 0.04% latency, 94.67 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.44 ms, 0.56% latency, 18.05 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.55 us, 0.02% latency, 65.14 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 661.61 us, 0.26% latency, 12.32 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 103.47 us, 0.04% latency, 59.09 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 83.68 us, 0.03% latency, 24.35 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.46 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.09 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 56.51 us, 0.02% latency, 64.04 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 408.89 us, 0.16% latency, 43.51 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 111.1 us, 0.04% latency, 80.04 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 45.54 us, 0.02% latency, 69.35 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 93.7 us, 0.04% latency, 94.91 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.43 ms, 0.56% latency, 18.14 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 641.35 us, 0.25% latency, 12.71 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 101.57 us, 0.04% latency, 60.2 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.46 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.73 us, 0.03% latency, 24.63 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.07 us, 0.02% latency, 65.7 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 431.3 us, 0.17% latency, 41.25 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 127.32 us, 0.05% latency, 69.85 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 50.31 us, 0.02% latency, 62.78 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 95.13 us, 0.04% latency, 93.48 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.39 ms, 0.54% latency, 18.67 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.88 us, 0.02% latency, 67.16 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 635.62 us, 0.25% latency, 12.83 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.37 us, 0.04% latency, 60.91 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.7 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 81.78 us, 0.03% latency, 24.92 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.8 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.6 us, 0.02% latency, 66.28 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 398.87 us, 0.16% latency, 44.6 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 109.43 us, 0.04% latency, 81.26 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.63 us, 0.02% latency, 72.38 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.03 us, 0.04% latency, 96.63 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.39 ms, 0.54% latency, 18.68 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 631.81 us, 0.25% latency, 12.9 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.61 us, 0.04% latency, 60.77 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.23 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 81.3 us, 0.03% latency, 25.07 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 27.66 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.88 us, 0.02% latency, 67.16 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 403.17 us, 0.16% latency, 44.12 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 111.58 us, 0.04% latency, 79.7 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.11 us, 0.02% latency, 71.6 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 93.7 us, 0.04% latency, 94.91 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 23.37 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.41 ms, 0.55% latency, 18.39 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.17 us, 0.02% latency, 68.06 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 634.43 us, 0.25% latency, 12.85 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 99.42 us, 0.04% latency, 61.5 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 27.42 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.49 us, 0.03% latency, 24.7 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 400.54 us, 0.16% latency, 44.41 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 112.06 us, 0.04% latency, 79.36 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.87 us, 0.02% latency, 71.99 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.51 us, 0.04% latency, 96.13 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 23.84 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.4 ms, 0.55% latency, 18.57 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.17 us, 0.02% latency, 68.06 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 636.82 us, 0.25% latency, 12.8 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 101.8 us, 0.04% latency, 60.06 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.94 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.25 us, 0.03% latency, 24.78 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 405.79 us, 0.16% latency, 43.84 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 111.1 us, 0.04% latency, 80.04 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.11 us, 0.02% latency, 71.6 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.74 us, 0.04% latency, 95.89 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.38 ms, 0.54% latency, 18.75 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 52.93 us, 0.02% latency, 68.37 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 630.14 us, 0.25% latency, 12.94 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.85 us, 0.04% latency, 60.62 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.46 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 81.3 us, 0.03% latency, 25.07 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 27.89 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.36 us, 0.02% latency, 66.57 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 401.74 us, 0.16% latency, 44.28 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.63 us, 0.04% latency, 80.39 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.58 us, 0.02% latency, 70.83 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.03 us, 0.04% latency, 96.63 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 23.84 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.41 ms, 0.55% latency, 18.38 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 52.93 us, 0.02% latency, 68.37 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 658.27 us, 0.26% latency, 12.38 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.14 us, 0.04% latency, 61.06 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.23 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 83.45 us, 0.03% latency, 24.42 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.51 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 27.89 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.07 us, 0.02% latency, 65.7 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 402.45 us, 0.16% latency, 44.2 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 111.1 us, 0.04% latency, 80.04 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.63 us, 0.02% latency, 72.38 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 93.7 us, 0.04% latency, 94.91 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.39 ms, 0.55% latency, 18.65 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 52.69 us, 0.02% latency, 68.68 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 633.24 us, 0.25% latency, 12.87 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 99.66 us, 0.04% latency, 61.35 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.23 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.49 us, 0.03% latency, 24.7 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 23.84 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.64 us, 0.02% latency, 67.45 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 407.93 us, 0.16% latency, 43.61 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 109.43 us, 0.04% latency, 81.26 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.11 us, 0.02% latency, 71.6 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 94.89 us, 0.04% latency, 93.72 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.38 ms, 0.54% latency, 18.78 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.41 us, 0.02% latency, 67.76 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 627.76 us, 0.25% latency, 12.99 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 99.9 us, 0.04% latency, 61.2 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.51 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.02 us, 0.03% latency, 24.85 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 52.69 us, 0.02% latency, 68.68 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 400.78 us, 0.16% latency, 44.39 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 109.91 us, 0.04% latency, 80.91 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.87 us, 0.02% latency, 71.99 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.03 us, 0.04% latency, 96.63 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (12): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.42 ms, 0.56% latency, 18.32 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 51.26 us, 0.02% latency, 70.59 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 662.33 us, 0.26% latency, 12.31 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 121.59 us, 0.05% latency, 50.28 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.7 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 84.4 us, 0.03% latency, 24.15 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.17 us, 0.02% latency, 68.06 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 401.74 us, 0.16% latency, 44.28 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.39 us, 0.04% latency, 80.56 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.39 us, 0.02% latency, 72.78 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 93.22 us, 0.04% latency, 95.4 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (13): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.39 ms, 0.55% latency, 18.62 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.64 us, 0.02% latency, 67.45 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 638.01 us, 0.25% latency, 12.78 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 101.57 us, 0.04% latency, 60.2 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 27.18 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.25 us, 0.03% latency, 24.78 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 23.84 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.64 us, 0.02% latency, 67.45 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 400.78 us, 0.16% latency, 44.39 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.15 us, 0.04% latency, 80.74 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.39 us, 0.02% latency, 72.78 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.98 us, 0.04% latency, 95.64 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 23.37 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (14): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.41 ms, 0.55% latency, 18.35 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.6 us, 0.02% latency, 66.28 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 633.48 us, 0.25% latency, 12.87 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 99.42 us, 0.04% latency, 61.5 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.46 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.02 us, 0.03% latency, 24.85 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.36 us, 0.02% latency, 66.57 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 402.45 us, 0.16% latency, 44.2 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.39 us, 0.04% latency, 80.56 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.35 us, 0.02% latency, 71.21 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.51 us, 0.04% latency, 96.13 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (15): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.39 ms, 0.55% latency, 18.65 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.31 us, 0.02% latency, 65.42 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 633.96 us, 0.25% latency, 12.86 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 102.04 us, 0.04% latency, 59.92 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.23 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.25 us, 0.03% latency, 24.78 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.33 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.64 us, 0.02% latency, 67.45 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 401.02 us, 0.16% latency, 44.36 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 109.43 us, 0.04% latency, 81.26 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.63 us, 0.02% latency, 72.38 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.74 us, 0.04% latency, 95.89 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (16): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.39 ms, 0.54% latency, 18.68 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.64 us, 0.02% latency, 67.45 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 632.52 us, 0.25% latency, 12.89 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.14 us, 0.04% latency, 61.06 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.23 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 83.45 us, 0.03% latency, 24.42 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.31 us, 0.02% latency, 65.42 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 402.93 us, 0.16% latency, 44.15 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 109.67 us, 0.04% latency, 81.09 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.35 us, 0.02% latency, 71.21 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.98 us, 0.04% latency, 95.64 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (17): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.42 ms, 0.55% latency, 18.33 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 633.0 us, 0.25% latency, 12.88 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 101.8 us, 0.04% latency, 60.06 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.99 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.25 us, 0.03% latency, 24.78 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 52.69 us, 0.02% latency, 68.68 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 431.06 us, 0.17% latency, 41.27 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 109.91 us, 0.04% latency, 80.91 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 63.66 us, 0.02% latency, 49.61 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 97.75 us, 0.04% latency, 90.98 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 23.84 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (18): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.39 ms, 0.55% latency, 18.62 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 634.91 us, 0.25% latency, 12.84 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 99.66 us, 0.04% latency, 61.35 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.99 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 81.78 us, 0.03% latency, 24.92 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.09 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 400.07 us, 0.16% latency, 44.47 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 109.91 us, 0.04% latency, 80.91 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.63 us, 0.02% latency, 72.38 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.51 us, 0.04% latency, 96.13 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 23.84 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (19): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.39 ms, 0.55% latency, 18.64 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 52.93 us, 0.02% latency, 68.37 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 635.62 us, 0.25% latency, 12.83 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.61 us, 0.04% latency, 60.77 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.7 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 83.45 us, 0.03% latency, 24.42 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.27 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.13 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.36 us, 0.02% latency, 66.57 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 403.17 us, 0.16% latency, 44.12 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.39 us, 0.04% latency, 80.56 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.58 us, 0.02% latency, 70.83 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.51 us, 0.04% latency, 96.13 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (20): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.43 ms, 0.56% latency, 18.21 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.64 us, 0.02% latency, 67.45 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 634.43 us, 0.25% latency, 12.85 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.37 us, 0.04% latency, 60.91 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.46 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.73 us, 0.03% latency, 24.63 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 56.27 us, 0.02% latency, 64.31 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 408.17 us, 0.16% latency, 43.58 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 111.34 us, 0.04% latency, 79.87 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.58 us, 0.02% latency, 70.83 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 93.46 us, 0.04% latency, 95.15 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (21): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.4 ms, 0.55% latency, 18.54 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.07 us, 0.02% latency, 65.7 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 636.82 us, 0.25% latency, 12.8 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.37 us, 0.04% latency, 60.91 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 27.66 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.25 us, 0.03% latency, 24.78 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.09 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.84 us, 0.02% latency, 65.99 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 402.93 us, 0.16% latency, 44.15 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.63 us, 0.04% latency, 80.39 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.11 us, 0.02% latency, 71.6 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 91.79 us, 0.04% latency, 96.88 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (22): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.39 ms, 0.55% latency, 18.66 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.88 us, 0.02% latency, 67.16 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 634.43 us, 0.25% latency, 12.85 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.37 us, 0.04% latency, 60.91 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.75 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.49 us, 0.03% latency, 24.7 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.13 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.41 us, 0.02% latency, 67.76 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 401.5 us, 0.16% latency, 44.31 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.39 us, 0.04% latency, 80.56 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.39 us, 0.02% latency, 72.78 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 93.46 us, 0.04% latency, 95.15 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (23): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.41 ms, 0.55% latency, 18.37 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.41 us, 0.02% latency, 67.76 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 656.37 us, 0.26% latency, 12.42 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.61 us, 0.04% latency, 60.77 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.46 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.49 us, 0.03% latency, 24.7 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.33 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 52.69 us, 0.02% latency, 68.68 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 401.5 us, 0.16% latency, 44.31 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 109.67 us, 0.04% latency, 81.09 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.11 us, 0.02% latency, 71.6 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 91.55 us, 0.04% latency, 97.13 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (24): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.4 ms, 0.55% latency, 18.49 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 52.93 us, 0.02% latency, 68.37 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 633.0 us, 0.25% latency, 12.88 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 98.71 us, 0.04% latency, 61.94 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.23 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 81.78 us, 0.03% latency, 24.92 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 52.93 us, 0.02% latency, 68.37 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 413.66 us, 0.16% latency, 43.0 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.86 us, 0.04% latency, 80.21 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.87 us, 0.02% latency, 71.99 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 94.65 us, 0.04% latency, 93.95 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (25): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.4 ms, 0.55% latency, 18.51 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.6 us, 0.02% latency, 66.28 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 637.53 us, 0.25% latency, 12.79 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.14 us, 0.04% latency, 61.06 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 27.42 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.49 us, 0.03% latency, 24.7 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.03 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.07 us, 0.02% latency, 65.7 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 403.88 us, 0.16% latency, 44.05 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.63 us, 0.04% latency, 80.39 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.58 us, 0.02% latency, 70.83 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.98 us, 0.04% latency, 95.64 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (26): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.42 ms, 0.56% latency, 18.24 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.17 us, 0.02% latency, 68.06 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 659.7 us, 0.26% latency, 12.36 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 120.4 us, 0.05% latency, 50.78 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.94 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 81.54 us, 0.03% latency, 24.99 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.03 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.6 us, 0.02% latency, 66.28 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 402.69 us, 0.16% latency, 44.18 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.63 us, 0.04% latency, 80.39 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.87 us, 0.02% latency, 71.99 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.74 us, 0.04% latency, 95.89 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (27): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.4 ms, 0.55% latency, 18.56 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 638.25 us, 0.25% latency, 12.77 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 101.33 us, 0.04% latency, 60.34 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.46 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.97 us, 0.03% latency, 24.56 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.56 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.36 us, 0.02% latency, 66.57 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 400.3 us, 0.16% latency, 44.44 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.39 us, 0.04% latency, 80.56 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.87 us, 0.02% latency, 71.99 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.74 us, 0.04% latency, 95.89 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (28): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.42 ms, 0.56% latency, 18.26 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 633.24 us, 0.25% latency, 12.87 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 99.9 us, 0.04% latency, 61.2 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.46 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 81.54 us, 0.03% latency, 24.99 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.03 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.55 us, 0.02% latency, 65.14 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 403.88 us, 0.16% latency, 44.05 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.86 us, 0.04% latency, 80.21 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.11 us, 0.02% latency, 71.6 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.98 us, 0.04% latency, 95.64 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.27 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (29): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.41 ms, 0.55% latency, 18.47 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.55 us, 0.02% latency, 65.14 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 639.44 us, 0.25% latency, 12.75 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 101.09 us, 0.04% latency, 60.48 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 27.66 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.73 us, 0.03% latency, 24.63 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.51 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.09 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.84 us, 0.02% latency, 65.99 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 406.5 us, 0.16% latency, 43.76 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.39 us, 0.04% latency, 80.56 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.35 us, 0.02% latency, 71.21 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 93.46 us, 0.04% latency, 95.15 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (30): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.39 ms, 0.54% latency, 18.71 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.6 us, 0.02% latency, 66.28 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 632.29 us, 0.25% latency, 12.89 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 99.18 us, 0.04% latency, 61.64 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.99 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 81.54 us, 0.03% latency, 24.99 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.09 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.64 us, 0.02% latency, 67.45 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 400.3 us, 0.16% latency, 44.44 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 108.72 us, 0.04% latency, 81.8 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.63 us, 0.02% latency, 72.38 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.03 us, 0.04% latency, 96.63 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (31): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.43 ms, 0.56% latency, 18.21 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.41 us, 0.02% latency, 67.76 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 634.19 us, 0.25% latency, 12.85 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.61 us, 0.04% latency, 60.77 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.7 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.49 us, 0.03% latency, 24.7 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.56 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.88 us, 0.02% latency, 67.16 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 432.97 us, 0.17% latency, 41.09 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 124.45 us, 0.05% latency, 71.46 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 52.45 us, 0.02% latency, 60.21 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 95.61 us, 0.04% latency, 93.02 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (32): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.4 ms, 0.55% latency, 18.59 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 639.92 us, 0.25% latency, 12.74 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 99.66 us, 0.04% latency, 61.35 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.7 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 83.92 us, 0.03% latency, 24.28 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 401.74 us, 0.16% latency, 44.28 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 108.96 us, 0.04% latency, 81.62 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.11 us, 0.02% latency, 71.6 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.51 us, 0.04% latency, 96.13 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (33): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.39 ms, 0.55% latency, 18.65 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.17 us, 0.02% latency, 68.06 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 632.05 us, 0.25% latency, 12.9 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 99.42 us, 0.04% latency, 61.5 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.46 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.49 us, 0.03% latency, 24.7 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.47 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.41 us, 0.02% latency, 67.76 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 402.21 us, 0.16% latency, 44.23 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.39 us, 0.04% latency, 80.56 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.11 us, 0.02% latency, 71.6 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.74 us, 0.04% latency, 95.89 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (34): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.41 ms, 0.55% latency, 18.37 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.17 us, 0.02% latency, 68.06 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 630.14 us, 0.25% latency, 12.94 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 99.42 us, 0.04% latency, 61.5 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.23 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 81.3 us, 0.03% latency, 25.07 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.36 us, 0.02% latency, 66.57 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 401.02 us, 0.16% latency, 44.36 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 111.82 us, 0.04% latency, 79.53 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.87 us, 0.02% latency, 71.99 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.98 us, 0.04% latency, 95.64 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 23.84 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (35): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.4 ms, 0.55% latency, 18.53 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 639.2 us, 0.25% latency, 12.75 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 100.37 us, 0.04% latency, 60.91 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.46 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 81.06 us, 0.03% latency, 25.14 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.37 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 66.86 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 401.97 us, 0.16% latency, 44.25 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 109.67 us, 0.04% latency, 81.09 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.87 us, 0.02% latency, 71.99 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 92.03 us, 0.04% latency, 96.63 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (36): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.39 ms, 0.55% latency, 18.66 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.17 us, 0.02% latency, 68.06 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 636.82 us, 0.25% latency, 12.8 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 99.66 us, 0.04% latency, 61.35 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 27.66 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 81.78 us, 0.03% latency, 24.92 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.03 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.09 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.6 us, 0.02% latency, 66.28 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 398.64 us, 0.16% latency, 44.62 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 109.67 us, 0.04% latency, 81.09 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.87 us, 0.02% latency, 71.99 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 93.22 us, 0.04% latency, 95.4 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 23.13 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (37): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.42 ms, 0.56% latency, 18.26 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.17 us, 0.02% latency, 68.06 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 662.09 us, 0.26% latency, 12.31 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 99.66 us, 0.04% latency, 61.35 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.94 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 83.45 us, 0.03% latency, 24.42 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.75 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.56 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.84 us, 0.02% latency, 65.99 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 402.69 us, 0.16% latency, 44.18 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 110.15 us, 0.04% latency, 80.74 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.58 us, 0.02% latency, 70.83 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 93.22 us, 0.04% latency, 95.4 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (38): Block(\n",
      "        25.25 M, 0.32% Params, 12.97 GMACs, 0.92% MACs, 1.4 ms, 0.55% latency, 18.53 TFLOPS, \n",
      "        (norm1): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 52.93 us, 0.02% latency, 68.37 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          7.93 M, 0.10% Params, 4.08 GMACs, 0.29% MACs, 634.91 us, 0.25% latency, 12.84 TFLOPS, \n",
      "          (qkv): Linear(5.95 M, 0.08% Params, 3.06 GMACs, 0.22% MACs, 101.09 us, 0.04% latency, 60.48 TFLOPS, in_features=1408, out_features=4224, bias=False)\n",
      "          (attn_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.7 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "          (proj): Linear(1.98 M, 0.03% Params, 1.02 GMACs, 0.07% MACs, 82.25 us, 0.03% latency, 24.78 TFLOPS, in_features=1408, out_features=1408, bias=True)\n",
      "          (proj_drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.8 us, 0.01% latency, 0.0 FLOPS, )\n",
      "        (norm2): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.41 us, 0.02% latency, 67.76 GFLOPS, (1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          17.31 M, 0.22% Params, 8.89 GMACs, 0.63% MACs, 409.13 us, 0.16% latency, 43.48 TFLOPS, \n",
      "          (fc1): Linear(8.66 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 109.43 us, 0.04% latency, 81.26 TFLOPS, in_features=1408, out_features=6144, bias=True)\n",
      "          (act): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 51.02 us, 0.02% latency, 61.9 GFLOPS, approximate='none')\n",
      "          (fc2): Linear(8.65 M, 0.11% Params, 4.45 GMACs, 0.32% MACs, 93.46 us, 0.04% latency, 95.15 TFLOPS, in_features=6144, out_features=1408, bias=True)\n",
      "          (drop): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_vision): LayerNorm(2.82 k, 0.00% Params, 0 MACs, 0.00% MACs, 62.94 us, 0.02% latency, 57.49 GFLOPS, (1408,), eps=1e-05, elementwise_affine=True)\n",
      "  (Qformer): BertLMHeadModel(\n",
      "    105.14 M, 1.34% Params, 12.75 GMACs, 0.91% MACs, 31.37 ms, 12.30% latency, 813.11 GFLOPS, \n",
      "    (bert): BertModel(\n",
      "      105.14 M, 1.34% Params, 12.75 GMACs, 0.91% MACs, 31.37 ms, 12.30% latency, 813.11 GFLOPS, \n",
      "      (embeddings): BertEmbeddings(\n",
      "        1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 183.11 us, 0.07% latency, 1.34 GFLOPS, \n",
      "        (word_embeddings): None\n",
      "        (position_embeddings): None\n",
      "        (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 59.84 us, 0.02% latency, 4.11 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 23.13 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        105.14 M, 1.34% Params, 12.75 GMACs, 0.91% MACs, 30.85 ms, 12.10% latency, 826.71 GFLOPS, \n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            10.43 M, 0.13% Params, 1.67 GMACs, 0.12% MACs, 3.26 ms, 1.28% latency, 1.02 TFLOPS, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 0.03% Params, 154.14 MMACs, 0.01% MACs, 1.28 ms, 0.50% latency, 241.9 GFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                1.77 M, 0.02% Params, 116.39 MMACs, 0.01% MACs, 842.57 us, 0.33% latency, 276.31 GFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 111.34 us, 0.04% latency, 678.07 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 132.8 us, 0.05% latency, 568.51 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 113.96 us, 0.04% latency, 662.47 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 26.23 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 325.44 us, 0.13% latency, 232.74 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 114.44 us, 0.04% latency, 659.71 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 56.51 us, 0.02% latency, 4.35 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.51 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (crossattention): BertAttention(\n",
      "              3.35 M, 0.04% Params, 1.21 GMACs, 0.09% MACs, 1.2 ms, 0.47% latency, 2.02 TFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                2.75 M, 0.04% Params, 1.17 GMACs, 0.08% MACs, 798.23 us, 0.31% latency, 2.94 TFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.96 us, 0.04% latency, 692.91 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(1.08 M, 0.01% Params, 555.81 MMACs, 0.04% MACs, 114.92 us, 0.05% latency, 9.67 TFLOPS, in_features=1408, out_features=768, bias=True)\n",
      "                (value): Linear(1.08 M, 0.01% Params, 555.81 MMACs, 0.04% MACs, 106.1 us, 0.04% latency, 10.48 TFLOPS, in_features=1408, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.75 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 322.82 us, 0.13% latency, 234.63 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 110.39 us, 0.04% latency, 683.93 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 56.03 us, 0.02% latency, 4.39 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.75 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): None\n",
      "            (output): None\n",
      "            (intermediate_query): BertIntermediate(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 245.09 us, 0.10% latency, 1.23 TFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 122.55 us, 0.05% latency, 2.46 TFLOPS, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 41.01 us, 0.02% latency, 0.0 FLOPS, )\n",
      "            )\n",
      "            (output_query): BertOutput(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 308.99 us, 0.12% latency, 978.14 GFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 105.62 us, 0.04% latency, 2.86 TFLOPS, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.07 us, 0.02% latency, 4.46 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            7.09 M, 0.09% Params, 456.13 MMACs, 0.03% MACs, 1.92 ms, 0.75% latency, 474.29 GFLOPS, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 0.03% Params, 154.14 MMACs, 0.01% MACs, 1.19 ms, 0.46% latency, 260.34 GFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                1.77 M, 0.02% Params, 116.39 MMACs, 0.01% MACs, 789.4 us, 0.31% latency, 294.92 GFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 107.29 us, 0.04% latency, 703.69 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 113.49 us, 0.04% latency, 665.25 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.72 us, 0.04% latency, 694.43 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 315.9 us, 0.12% latency, 239.77 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.24 us, 0.04% latency, 697.49 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.84 us, 0.02% latency, 4.48 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.99 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): None\n",
      "            (output): None\n",
      "            (intermediate_query): BertIntermediate(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 237.7 us, 0.09% latency, 1.27 TFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 120.88 us, 0.05% latency, 2.5 TFLOPS, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 39.1 us, 0.02% latency, 0.0 FLOPS, )\n",
      "            )\n",
      "            (output_query): BertOutput(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 338.55 us, 0.13% latency, 892.72 GFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 128.27 us, 0.05% latency, 2.35 TFLOPS, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.31 us, 0.02% latency, 4.44 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.99 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            10.43 M, 0.13% Params, 1.67 GMACs, 0.12% MACs, 3.13 ms, 1.23% latency, 1.07 TFLOPS, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 0.03% Params, 154.14 MMACs, 0.01% MACs, 1.2 ms, 0.47% latency, 257.39 GFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                1.77 M, 0.02% Params, 116.39 MMACs, 0.01% MACs, 797.99 us, 0.31% latency, 291.74 GFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 106.1 us, 0.04% latency, 711.59 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 116.11 us, 0.05% latency, 650.22 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 109.43 us, 0.04% latency, 689.89 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 319.48 us, 0.13% latency, 237.08 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.0 us, 0.04% latency, 699.03 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.31 us, 0.02% latency, 4.44 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.27 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (crossattention): BertAttention(\n",
      "              3.35 M, 0.04% Params, 1.21 GMACs, 0.09% MACs, 1.18 ms, 0.46% latency, 2.05 TFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                2.75 M, 0.04% Params, 1.17 GMACs, 0.08% MACs, 787.02 us, 0.31% latency, 2.99 TFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.48 us, 0.04% latency, 695.95 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(1.08 M, 0.01% Params, 555.81 MMACs, 0.04% MACs, 114.44 us, 0.04% latency, 9.71 TFLOPS, in_features=1408, out_features=768, bias=True)\n",
      "                (value): Linear(1.08 M, 0.01% Params, 555.81 MMACs, 0.04% MACs, 105.38 us, 0.04% latency, 10.55 TFLOPS, in_features=1408, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.51 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 315.9 us, 0.12% latency, 239.77 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 107.77 us, 0.04% latency, 700.57 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.31 us, 0.02% latency, 4.44 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.75 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): None\n",
      "            (output): None\n",
      "            (intermediate_query): BertIntermediate(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 236.51 us, 0.09% latency, 1.28 TFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 119.45 us, 0.05% latency, 2.53 TFLOPS, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 39.1 us, 0.02% latency, 0.0 FLOPS, )\n",
      "            )\n",
      "            (output_query): BertOutput(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 311.14 us, 0.12% latency, 971.39 GFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 104.43 us, 0.04% latency, 2.89 TFLOPS, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.36 us, 0.02% latency, 4.52 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.51 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            7.09 M, 0.09% Params, 456.13 MMACs, 0.03% MACs, 1.95 ms, 0.77% latency, 467.46 GFLOPS, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 0.03% Params, 154.14 MMACs, 0.01% MACs, 1.24 ms, 0.49% latency, 249.4 GFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                1.77 M, 0.02% Params, 116.39 MMACs, 0.01% MACs, 840.43 us, 0.33% latency, 277.01 GFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 137.81 us, 0.05% latency, 547.85 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 115.63 us, 0.05% latency, 652.91 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 109.43 us, 0.04% latency, 689.89 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.99 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 316.62 us, 0.12% latency, 239.22 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 107.53 us, 0.04% latency, 702.13 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 4.54 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): None\n",
      "            (output): None\n",
      "            (intermediate_query): BertIntermediate(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 239.61 us, 0.09% latency, 1.26 TFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 122.07 us, 0.05% latency, 2.47 TFLOPS, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 38.86 us, 0.02% latency, 0.0 FLOPS, )\n",
      "            )\n",
      "            (output_query): BertOutput(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 310.9 us, 0.12% latency, 972.14 GFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 104.67 us, 0.04% latency, 2.89 TFLOPS, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.07 us, 0.02% latency, 4.46 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.51 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            10.43 M, 0.13% Params, 1.67 GMACs, 0.12% MACs, 3.12 ms, 1.22% latency, 1.07 TFLOPS, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 0.03% Params, 154.14 MMACs, 0.01% MACs, 1.2 ms, 0.47% latency, 258.16 GFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                1.77 M, 0.02% Params, 116.39 MMACs, 0.01% MACs, 798.23 us, 0.31% latency, 291.66 GFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 106.57 us, 0.04% latency, 708.41 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 115.87 us, 0.05% latency, 651.56 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 110.63 us, 0.04% latency, 682.46 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.99 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 318.05 us, 0.12% latency, 238.15 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.96 us, 0.04% latency, 692.91 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 4.54 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.27 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (crossattention): BertAttention(\n",
      "              3.35 M, 0.04% Params, 1.21 GMACs, 0.09% MACs, 1.18 ms, 0.46% latency, 2.06 TFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                2.75 M, 0.04% Params, 1.17 GMACs, 0.08% MACs, 782.97 us, 0.31% latency, 3.0 TFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 107.29 us, 0.04% latency, 703.69 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(1.08 M, 0.01% Params, 555.81 MMACs, 0.04% MACs, 112.06 us, 0.04% latency, 9.92 TFLOPS, in_features=1408, out_features=768, bias=True)\n",
      "                (value): Linear(1.08 M, 0.01% Params, 555.81 MMACs, 0.04% MACs, 105.14 us, 0.04% latency, 10.57 TFLOPS, in_features=1408, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.75 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 313.52 us, 0.12% latency, 241.59 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 107.77 us, 0.04% latency, 700.57 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.64 us, 0.02% latency, 4.58 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.03 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): None\n",
      "            (output): None\n",
      "            (intermediate_query): BertIntermediate(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 237.46 us, 0.09% latency, 1.27 TFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 121.83 us, 0.05% latency, 2.48 TFLOPS, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 38.62 us, 0.02% latency, 0.0 FLOPS, )\n",
      "            )\n",
      "            (output_query): BertOutput(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 304.46 us, 0.12% latency, 992.69 GFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 102.76 us, 0.04% latency, 2.94 TFLOPS, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.88 us, 0.02% latency, 4.56 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            7.09 M, 0.09% Params, 456.13 MMACs, 0.03% MACs, 1.88 ms, 0.74% latency, 484.98 GFLOPS, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 0.03% Params, 154.14 MMACs, 0.01% MACs, 1.18 ms, 0.46% latency, 261.34 GFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                1.77 M, 0.02% Params, 116.39 MMACs, 0.01% MACs, 787.97 us, 0.31% latency, 295.45 GFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 105.62 us, 0.04% latency, 714.81 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 114.68 us, 0.04% latency, 658.34 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.96 us, 0.04% latency, 692.91 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 313.76 us, 0.12% latency, 241.41 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 107.05 us, 0.04% latency, 705.25 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 4.54 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.51 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): None\n",
      "            (output): None\n",
      "            (intermediate_query): BertIntermediate(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 237.46 us, 0.09% latency, 1.27 TFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 119.45 us, 0.05% latency, 2.53 TFLOPS, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 38.86 us, 0.02% latency, 0.0 FLOPS, )\n",
      "            )\n",
      "            (output_query): BertOutput(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 305.41 us, 0.12% latency, 989.59 GFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 102.52 us, 0.04% latency, 2.95 TFLOPS, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.88 us, 0.02% latency, 4.56 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.08 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            10.43 M, 0.13% Params, 1.67 GMACs, 0.12% MACs, 3.11 ms, 1.22% latency, 1.07 TFLOPS, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 0.03% Params, 154.14 MMACs, 0.01% MACs, 1.18 ms, 0.46% latency, 262.19 GFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                1.77 M, 0.02% Params, 116.39 MMACs, 0.01% MACs, 785.35 us, 0.31% latency, 296.44 GFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 106.81 us, 0.04% latency, 706.83 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 113.73 us, 0.04% latency, 663.86 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 109.91 us, 0.04% latency, 686.9 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.32 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 313.52 us, 0.12% latency, 241.59 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.0 us, 0.04% latency, 699.03 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 4.54 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (crossattention): BertAttention(\n",
      "              3.35 M, 0.04% Params, 1.21 GMACs, 0.09% MACs, 1.2 ms, 0.47% latency, 2.02 TFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                2.75 M, 0.04% Params, 1.17 GMACs, 0.08% MACs, 809.43 us, 0.32% latency, 2.9 TFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.72 us, 0.04% latency, 694.43 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(1.08 M, 0.01% Params, 555.81 MMACs, 0.04% MACs, 132.32 us, 0.05% latency, 8.4 TFLOPS, in_features=1408, out_features=768, bias=True)\n",
      "                (value): Linear(1.08 M, 0.01% Params, 555.81 MMACs, 0.04% MACs, 108.24 us, 0.04% latency, 10.27 TFLOPS, in_features=1408, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 311.85 us, 0.12% latency, 242.88 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 106.57 us, 0.04% latency, 708.41 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 4.54 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): None\n",
      "            (output): None\n",
      "            (intermediate_query): BertIntermediate(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 232.46 us, 0.09% latency, 1.3 TFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 116.59 us, 0.05% latency, 2.59 TFLOPS, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 38.62 us, 0.02% latency, 0.0 FLOPS, )\n",
      "            )\n",
      "            (output_query): BertOutput(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 307.56 us, 0.12% latency, 982.69 GFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 103.0 us, 0.04% latency, 2.93 TFLOPS, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.88 us, 0.02% latency, 4.56 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            7.09 M, 0.09% Params, 456.13 MMACs, 0.03% MACs, 1.9 ms, 0.74% latency, 481.02 GFLOPS, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 0.03% Params, 154.14 MMACs, 0.01% MACs, 1.19 ms, 0.47% latency, 258.68 GFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                1.77 M, 0.02% Params, 116.39 MMACs, 0.01% MACs, 800.61 us, 0.31% latency, 290.79 GFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 110.15 us, 0.04% latency, 685.41 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 115.16 us, 0.05% latency, 655.61 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 116.11 us, 0.05% latency, 650.22 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.99 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 313.28 us, 0.12% latency, 241.77 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 106.33 us, 0.04% latency, 710.0 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.84 us, 0.02% latency, 4.48 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): None\n",
      "            (output): None\n",
      "            (intermediate_query): BertIntermediate(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 236.03 us, 0.09% latency, 1.28 TFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 120.4 us, 0.05% latency, 2.51 TFLOPS, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 38.86 us, 0.02% latency, 0.0 FLOPS, )\n",
      "            )\n",
      "            (output_query): BertOutput(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 307.56 us, 0.12% latency, 982.69 GFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 104.9 us, 0.04% latency, 2.88 TFLOPS, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.17 us, 0.02% latency, 4.62 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            10.43 M, 0.13% Params, 1.67 GMACs, 0.12% MACs, 3.13 ms, 1.23% latency, 1.07 TFLOPS, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 0.03% Params, 154.14 MMACs, 0.01% MACs, 1.22 ms, 0.48% latency, 253.86 GFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                1.77 M, 0.02% Params, 116.39 MMACs, 0.01% MACs, 820.4 us, 0.32% latency, 283.77 GFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 111.34 us, 0.04% latency, 678.07 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 114.44 us, 0.04% latency, 659.71 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 133.28 us, 0.05% latency, 566.47 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.03 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 317.81 us, 0.12% latency, 238.33 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.48 us, 0.04% latency, 695.95 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.6 us, 0.02% latency, 4.5 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.75 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (crossattention): BertAttention(\n",
      "              3.35 M, 0.04% Params, 1.21 GMACs, 0.09% MACs, 1.18 ms, 0.46% latency, 2.05 TFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                2.75 M, 0.04% Params, 1.17 GMACs, 0.08% MACs, 782.25 us, 0.31% latency, 3.0 TFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.96 us, 0.04% latency, 692.91 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(1.08 M, 0.01% Params, 555.81 MMACs, 0.04% MACs, 113.49 us, 0.04% latency, 9.8 TFLOPS, in_features=1408, out_features=768, bias=True)\n",
      "                (value): Linear(1.08 M, 0.01% Params, 555.81 MMACs, 0.04% MACs, 103.47 us, 0.04% latency, 10.74 TFLOPS, in_features=1408, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 318.05 us, 0.12% latency, 238.15 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.0 us, 0.04% latency, 699.03 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 55.31 us, 0.02% latency, 4.44 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): None\n",
      "            (output): None\n",
      "            (intermediate_query): BertIntermediate(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 235.56 us, 0.09% latency, 1.28 TFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 118.02 us, 0.05% latency, 2.56 TFLOPS, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 39.34 us, 0.02% latency, 0.0 FLOPS, )\n",
      "            )\n",
      "            (output_query): BertOutput(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 305.89 us, 0.12% latency, 988.05 GFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 103.24 us, 0.04% latency, 2.93 TFLOPS, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.41 us, 0.02% latency, 4.6 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.03 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            7.09 M, 0.09% Params, 456.13 MMACs, 0.03% MACs, 1.91 ms, 0.75% latency, 478.26 GFLOPS, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 0.03% Params, 154.14 MMACs, 0.01% MACs, 1.2 ms, 0.47% latency, 257.19 GFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                1.77 M, 0.02% Params, 116.39 MMACs, 0.01% MACs, 782.49 us, 0.31% latency, 297.52 GFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 104.9 us, 0.04% latency, 719.68 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 112.3 us, 0.04% latency, 672.31 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 109.2 us, 0.04% latency, 691.4 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.8 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 338.55 us, 0.13% latency, 223.73 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 108.48 us, 0.04% latency, 695.95 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 73.91 us, 0.03% latency, 3.33 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.75 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): None\n",
      "            (output): None\n",
      "            (intermediate_query): BertIntermediate(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 240.09 us, 0.09% latency, 1.26 TFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 123.5 us, 0.05% latency, 2.45 TFLOPS, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 38.62 us, 0.02% latency, 0.0 FLOPS, )\n",
      "            )\n",
      "            (output_query): BertOutput(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 308.51 us, 0.12% latency, 979.65 GFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 103.0 us, 0.04% latency, 2.93 TFLOPS, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.84 us, 0.02% latency, 4.48 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.51 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            10.43 M, 0.13% Params, 1.67 GMACs, 0.12% MACs, 3.08 ms, 1.21% latency, 1.08 TFLOPS, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 0.03% Params, 154.14 MMACs, 0.01% MACs, 1.18 ms, 0.46% latency, 261.13 GFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                1.77 M, 0.02% Params, 116.39 MMACs, 0.01% MACs, 789.4 us, 0.31% latency, 294.92 GFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 107.29 us, 0.04% latency, 703.69 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 114.68 us, 0.04% latency, 658.34 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 109.91 us, 0.04% latency, 686.9 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.03 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 314.24 us, 0.12% latency, 241.04 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 107.77 us, 0.04% latency, 700.57 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.88 us, 0.02% latency, 4.56 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.27 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (crossattention): BertAttention(\n",
      "              3.35 M, 0.04% Params, 1.21 GMACs, 0.09% MACs, 1.17 ms, 0.46% latency, 2.08 TFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                2.75 M, 0.04% Params, 1.17 GMACs, 0.08% MACs, 777.01 us, 0.30% latency, 3.02 TFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 107.05 us, 0.04% latency, 705.25 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(1.08 M, 0.01% Params, 555.81 MMACs, 0.04% MACs, 113.73 us, 0.04% latency, 9.77 TFLOPS, in_features=1408, out_features=768, bias=True)\n",
      "                (value): Linear(1.08 M, 0.01% Params, 555.81 MMACs, 0.04% MACs, 103.24 us, 0.04% latency, 10.77 TFLOPS, in_features=1408, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.51 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 314.24 us, 0.12% latency, 241.04 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 107.29 us, 0.04% latency, 703.69 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 4.54 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): None\n",
      "            (output): None\n",
      "            (intermediate_query): BertIntermediate(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 236.99 us, 0.09% latency, 1.27 TFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 118.49 us, 0.05% latency, 2.55 TFLOPS, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 38.86 us, 0.02% latency, 0.0 FLOPS, )\n",
      "            )\n",
      "            (output_query): BertOutput(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 308.75 us, 0.12% latency, 978.89 GFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 103.95 us, 0.04% latency, 2.91 TFLOPS, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 53.88 us, 0.02% latency, 4.56 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.27 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            7.09 M, 0.09% Params, 456.13 MMACs, 0.03% MACs, 1.91 ms, 0.75% latency, 477.25 GFLOPS, \n",
      "            (attention): BertAttention(\n",
      "              2.36 M, 0.03% Params, 154.14 MMACs, 0.01% MACs, 1.22 ms, 0.48% latency, 253.91 GFLOPS, \n",
      "              (self): BertSelfAttention(\n",
      "                1.77 M, 0.02% Params, 116.39 MMACs, 0.01% MACs, 826.36 us, 0.32% latency, 281.73 GFLOPS, \n",
      "                (query): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 106.81 us, 0.04% latency, 706.83 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 128.27 us, 0.05% latency, 588.59 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 111.34 us, 0.04% latency, 678.07 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.51 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                592.13 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 311.14 us, 0.12% latency, 243.44 GFLOPS, \n",
      "                (dense): Linear(590.59 k, 0.01% Params, 37.75 MMACs, 0.00% MACs, 107.05 us, 0.04% latency, 705.25 GFLOPS, in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 54.12 us, 0.02% latency, 4.54 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 25.03 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): None\n",
      "            (output): None\n",
      "            (intermediate_query): BertIntermediate(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 236.27 us, 0.09% latency, 1.28 TFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 119.92 us, 0.05% latency, 2.52 TFLOPS, in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 38.15 us, 0.01% latency, 0.0 FLOPS, )\n",
      "            )\n",
      "            (output_query): BertOutput(\n",
      "              2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 302.79 us, 0.12% latency, 998.16 GFLOPS, \n",
      "              (dense): Linear(2.36 M, 0.03% Params, 150.99 MMACs, 0.01% MACs, 102.28 us, 0.04% latency, 2.95 TFLOPS, in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(1.54 k, 0.00% Params, 0 MACs, 0.00% MACs, 52.93 us, 0.02% latency, 4.64 GFLOPS, (768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(0, 0.00% Params, 0 MACs, 0.00% MACs, 24.56 us, 0.01% latency, 0.0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): None\n",
      "  )\n",
      "  (llama_model): LlamaForCausalLM(\n",
      "    6738.42 M, 86.03% Params, 887.7 GMACs, 63.10% MACs, 160.29 ms, 62.85% latency, 11.08 TFLOPS, \n",
      "    (model): LlamaModel(\n",
      "      6607.35 M, 84.36% Params, 870.14 GMACs, 61.85% MACs, 158.32 ms, 62.07% latency, 10.99 TFLOPS, \n",
      "      (embed_tokens): Embedding(131.08 M, 1.67% Params, 0 MACs, 0.00% MACs, 238.42 us, 0.09% latency, 0.0 FLOPS, 32001, 4096, padding_idx=0)\n",
      "      (layers): ModuleList(\n",
      "        (0): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.57 ms, 1.79% latency, 11.9 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.28 ms, 0.89% latency, 7.97 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 349.76 us, 0.14% latency, 12.86 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 334.26 us, 0.13% latency, 13.45 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 347.85 us, 0.14% latency, 12.93 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 330.69 us, 0.13% latency, 13.6 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.82 ms, 0.71% latency, 19.92 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 533.34 us, 0.21% latency, 22.66 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 519.51 us, 0.20% latency, 23.26 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 530.48 us, 0.21% latency, 22.78 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 51.5 us, 0.02% latency, 28.64 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 135.9 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 127.32 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (1): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.94 ms, 1.94% latency, 11.0 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.65 ms, 1.04% latency, 6.85 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 330.69 us, 0.13% latency, 13.6 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 329.02 us, 0.13% latency, 13.67 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 349.52 us, 0.14% latency, 12.86 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 327.35 us, 0.13% latency, 13.74 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 84.64 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.83 ms, 0.72% latency, 19.82 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 554.56 us, 0.22% latency, 21.79 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 520.23 us, 0.20% latency, 23.23 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 522.61 us, 0.20% latency, 23.12 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 51.5 us, 0.02% latency, 28.64 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 126.84 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 127.08 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (2): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.9 ms, 1.92% latency, 11.11 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.41 ms, 0.95% latency, 7.51 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 336.41 us, 0.13% latency, 13.37 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 329.26 us, 0.13% latency, 13.66 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 332.59 us, 0.13% latency, 13.52 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 332.12 us, 0.13% latency, 13.54 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 83.92 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 2.0 ms, 0.78% latency, 18.12 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 527.62 us, 0.21% latency, 22.9 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 517.85 us, 0.20% latency, 23.33 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 540.73 us, 0.21% latency, 22.35 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 50.54 us, 0.02% latency, 29.18 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 127.32 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 126.36 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (3): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.93 ms, 1.93% latency, 11.02 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.63 ms, 1.03% latency, 6.91 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 333.31 us, 0.13% latency, 13.49 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 335.69 us, 0.13% latency, 13.39 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 327.35 us, 0.13% latency, 13.74 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 325.44 us, 0.13% latency, 13.82 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 83.92 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.85 ms, 0.72% latency, 19.62 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 557.66 us, 0.22% latency, 21.67 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 537.63 us, 0.21% latency, 22.48 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 526.19 us, 0.21% latency, 22.96 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 50.07 us, 0.02% latency, 29.46 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 127.32 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 125.41 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (4): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.98 ms, 1.95% latency, 10.92 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.68 ms, 1.05% latency, 6.78 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 337.6 us, 0.13% latency, 13.32 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 330.45 us, 0.13% latency, 13.61 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 331.4 us, 0.13% latency, 13.57 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 330.45 us, 0.13% latency, 13.61 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 83.21 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.83 ms, 0.72% latency, 19.78 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 530.96 us, 0.21% latency, 22.76 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 526.91 us, 0.21% latency, 22.93 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 526.91 us, 0.21% latency, 22.93 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 51.74 us, 0.02% latency, 28.51 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 125.17 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 129.46 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (5): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.95 ms, 1.94% latency, 10.98 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.49 ms, 0.97% latency, 7.29 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 336.89 us, 0.13% latency, 13.35 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 349.28 us, 0.14% latency, 12.87 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 348.09 us, 0.14% latency, 12.92 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 333.07 us, 0.13% latency, 13.5 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.82 ms, 0.71% latency, 19.89 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 525.47 us, 0.21% latency, 23.0 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 529.05 us, 0.21% latency, 22.84 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 530.48 us, 0.21% latency, 22.78 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 51.26 us, 0.02% latency, 28.78 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 128.03 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 293.25 us, 0.11% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (6): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 5.01 ms, 1.97% latency, 10.85 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.69 ms, 1.05% latency, 6.75 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 354.05 us, 0.14% latency, 12.7 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 331.88 us, 0.13% latency, 13.55 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 330.69 us, 0.13% latency, 13.6 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 329.97 us, 0.13% latency, 13.63 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.85 ms, 0.73% latency, 19.57 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 548.12 us, 0.21% latency, 22.05 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 528.81 us, 0.21% latency, 22.85 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 531.67 us, 0.21% latency, 22.73 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 50.78 us, 0.02% latency, 29.05 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 128.03 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 128.27 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (7): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 5.07 ms, 1.99% latency, 10.73 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.64 ms, 1.04% latency, 6.87 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 334.5 us, 0.13% latency, 13.44 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 332.83 us, 0.13% latency, 13.51 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 330.21 us, 0.13% latency, 13.62 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 301.36 us, 0.12% latency, 14.92 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.99 ms, 0.78% latency, 18.2 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 515.7 us, 0.20% latency, 23.43 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 523.81 us, 0.21% latency, 23.07 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 541.69 us, 0.21% latency, 22.31 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 217.2 us, 0.09% latency, 6.79 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 127.79 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 107.53 us, 0.04% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (8): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.34 ms, 1.70% latency, 12.54 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.18 ms, 0.85% latency, 8.32 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 304.7 us, 0.12% latency, 14.76 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 300.88 us, 0.12% latency, 14.94 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 300.17 us, 0.12% latency, 14.98 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 313.76 us, 0.12% latency, 14.33 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 73.43 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.7 ms, 0.66% latency, 21.38 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 494.96 us, 0.19% latency, 24.41 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 504.73 us, 0.20% latency, 23.94 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 496.86 us, 0.19% latency, 24.32 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.63 us, 0.02% latency, 33.81 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 151.63 us, 0.06% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 109.67 us, 0.04% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (9): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 5.07 ms, 1.99% latency, 10.73 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.71 ms, 1.06% latency, 6.68 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 330.21 us, 0.13% latency, 13.62 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 336.89 us, 0.13% latency, 13.35 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 329.97 us, 0.13% latency, 13.63 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 334.26 us, 0.13% latency, 13.45 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.92 ms, 0.75% latency, 18.9 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 532.39 us, 0.21% latency, 22.7 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 484.7 us, 0.19% latency, 24.93 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 522.14 us, 0.20% latency, 23.14 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 61.75 us, 0.02% latency, 23.89 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 107.77 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 127.08 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (10): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.72 ms, 1.85% latency, 11.52 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.42 ms, 0.95% latency, 7.51 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 318.05 us, 0.12% latency, 14.14 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 322.58 us, 0.13% latency, 13.94 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 301.12 us, 0.12% latency, 14.93 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 304.7 us, 0.12% latency, 14.76 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 71.76 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.81 ms, 0.71% latency, 20.06 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 531.91 us, 0.21% latency, 22.72 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 489.71 us, 0.19% latency, 24.68 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 557.42 us, 0.22% latency, 21.68 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 50.54 us, 0.02% latency, 29.18 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 109.2 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 136.61 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (11): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.57 ms, 1.79% latency, 11.9 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.44 ms, 0.96% latency, 7.44 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 341.42 us, 0.13% latency, 13.17 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 332.83 us, 0.13% latency, 13.51 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 336.17 us, 0.13% latency, 13.38 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 314.47 us, 0.12% latency, 14.3 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.69 ms, 0.66% latency, 21.47 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 498.53 us, 0.20% latency, 24.24 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 491.14 us, 0.19% latency, 24.6 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 495.91 us, 0.19% latency, 24.37 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.39 us, 0.02% latency, 33.99 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 135.66 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 108.0 us, 0.04% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (12): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.78 ms, 1.88% latency, 11.37 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.38 ms, 0.93% latency, 7.63 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 308.75 us, 0.12% latency, 14.56 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 302.31 us, 0.12% latency, 14.87 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 301.84 us, 0.12% latency, 14.9 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 303.03 us, 0.12% latency, 14.84 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 73.19 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.85 ms, 0.73% latency, 19.59 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 534.06 us, 0.21% latency, 22.63 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 530.48 us, 0.21% latency, 22.78 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 534.06 us, 0.21% latency, 22.63 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 55.79 us, 0.02% latency, 26.44 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 109.2 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 253.2 us, 0.10% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (13): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.86 ms, 1.90% latency, 11.19 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.72 ms, 1.07% latency, 6.66 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 335.45 us, 0.13% latency, 13.4 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 308.75 us, 0.12% latency, 14.56 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 299.93 us, 0.12% latency, 14.99 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 331.88 us, 0.13% latency, 13.55 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 104.43 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.7 ms, 0.67% latency, 21.27 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 501.39 us, 0.20% latency, 24.1 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 491.38 us, 0.19% latency, 24.59 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 512.36 us, 0.20% latency, 23.58 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.39 us, 0.02% latency, 33.99 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 127.79 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 108.72 us, 0.04% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (14): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.67 ms, 1.83% latency, 11.64 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.41 ms, 0.95% latency, 7.51 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 307.8 us, 0.12% latency, 14.61 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 340.94 us, 0.13% latency, 13.19 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 334.98 us, 0.13% latency, 13.42 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 300.65 us, 0.12% latency, 14.96 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.86 ms, 0.73% latency, 19.53 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 502.35 us, 0.20% latency, 24.05 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 511.17 us, 0.20% latency, 23.64 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 495.67 us, 0.19% latency, 24.38 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 182.39 us, 0.07% latency, 8.09 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 108.48 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 107.77 us, 0.04% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (15): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 5.03 ms, 1.97% latency, 10.81 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.75 ms, 1.08% latency, 6.59 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 310.42 us, 0.12% latency, 14.48 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 344.75 us, 0.14% latency, 13.04 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 338.79 us, 0.13% latency, 13.27 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 332.36 us, 0.13% latency, 13.53 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.83 ms, 0.72% latency, 19.79 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 533.82 us, 0.21% latency, 22.64 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 527.62 us, 0.21% latency, 22.9 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 528.34 us, 0.21% latency, 22.87 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 52.45 us, 0.02% latency, 28.12 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 108.24 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 129.22 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (16): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.87 ms, 1.91% latency, 11.17 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.51 ms, 0.99% latency, 7.22 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 338.32 us, 0.13% latency, 13.29 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 340.46 us, 0.13% latency, 13.21 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 325.2 us, 0.13% latency, 13.83 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 303.27 us, 0.12% latency, 14.83 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 73.91 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.91 ms, 0.75% latency, 18.95 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 499.01 us, 0.20% latency, 24.22 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 532.39 us, 0.21% latency, 22.7 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 499.96 us, 0.20% latency, 24.17 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.39 us, 0.02% latency, 33.99 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 129.22 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 109.2 us, 0.04% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (17): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.86 ms, 1.90% latency, 11.2 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.51 ms, 0.98% latency, 7.22 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 338.79 us, 0.13% latency, 13.27 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 339.75 us, 0.13% latency, 13.23 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 338.32 us, 0.13% latency, 13.29 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 336.41 us, 0.13% latency, 13.37 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.85 ms, 0.73% latency, 19.57 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 554.56 us, 0.22% latency, 21.79 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 524.04 us, 0.21% latency, 23.06 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 532.15 us, 0.21% latency, 22.71 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 52.21 us, 0.02% latency, 28.25 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 127.32 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 145.44 us, 0.06% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (18): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 5.28 ms, 2.07% latency, 10.29 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.93 ms, 1.15% latency, 6.19 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 336.65 us, 0.13% latency, 13.36 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 345.23 us, 0.14% latency, 13.02 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 334.5 us, 0.13% latency, 13.44 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 334.98 us, 0.13% latency, 13.42 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 214.82 us, 0.08% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.88 ms, 0.74% latency, 19.3 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 531.91 us, 0.21% latency, 22.72 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 525.95 us, 0.21% latency, 22.98 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 570.3 us, 0.22% latency, 21.19 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 51.26 us, 0.02% latency, 28.78 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 126.6 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 128.27 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (19): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 5.29 ms, 2.07% latency, 10.28 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.79 ms, 1.09% latency, 6.49 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 344.51 us, 0.14% latency, 13.05 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 358.34 us, 0.14% latency, 12.55 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 335.93 us, 0.13% latency, 13.38 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 334.5 us, 0.13% latency, 13.44 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 85.35 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.85 ms, 0.73% latency, 19.57 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 528.57 us, 0.21% latency, 22.86 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 574.11 us, 0.23% latency, 21.05 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 527.86 us, 0.21% latency, 22.89 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.39 us, 0.02% latency, 33.99 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 126.36 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.69 us, 0.12% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (20): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.72 ms, 1.85% latency, 11.53 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.52 ms, 0.99% latency, 7.19 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 340.22 us, 0.13% latency, 13.22 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 338.08 us, 0.13% latency, 13.3 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 334.74 us, 0.13% latency, 13.43 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 361.92 us, 0.14% latency, 12.42 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.76 ms, 0.69% latency, 20.59 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 500.92 us, 0.20% latency, 24.12 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 526.67 us, 0.21% latency, 22.94 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 516.18 us, 0.20% latency, 23.41 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 44.82 us, 0.02% latency, 32.91 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 127.55 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 108.0 us, 0.04% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (21): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 5.18 ms, 2.03% latency, 10.51 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.82 ms, 1.10% latency, 6.44 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 340.7 us, 0.13% latency, 13.2 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 357.63 us, 0.14% latency, 12.57 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 339.98 us, 0.13% latency, 13.22 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 339.27 us, 0.13% latency, 13.25 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.89 ms, 0.74% latency, 19.15 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 536.92 us, 0.21% latency, 22.51 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 492.1 us, 0.19% latency, 24.56 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 497.58 us, 0.20% latency, 24.29 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 193.83 us, 0.08% latency, 7.61 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 127.32 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 128.98 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (22): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.9 ms, 1.92% latency, 11.1 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.71 ms, 1.06% latency, 6.7 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 305.89 us, 0.12% latency, 14.7 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 307.32 us, 0.12% latency, 14.63 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 328.3 us, 0.13% latency, 13.7 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 334.74 us, 0.13% latency, 13.43 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.76 ms, 0.69% latency, 20.65 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 551.22 us, 0.22% latency, 21.92 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 493.53 us, 0.19% latency, 24.48 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 503.06 us, 0.20% latency, 24.02 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 42.92 us, 0.02% latency, 34.37 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 108.72 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 127.55 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (23): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.72 ms, 1.85% latency, 11.52 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.32 ms, 0.91% latency, 7.83 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 307.08 us, 0.12% latency, 14.64 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 311.85 us, 0.12% latency, 14.42 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 304.22 us, 0.12% latency, 14.78 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 334.74 us, 0.13% latency, 13.43 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 72.48 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.97 ms, 0.77% latency, 18.43 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 537.4 us, 0.21% latency, 22.49 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 491.38 us, 0.19% latency, 24.59 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 567.2 us, 0.22% latency, 21.3 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 51.26 us, 0.02% latency, 28.78 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 108.24 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 127.79 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (24): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.67 ms, 1.83% latency, 11.64 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.44 ms, 0.96% latency, 7.43 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 309.94 us, 0.12% latency, 14.51 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 307.08 us, 0.12% latency, 14.64 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 309.47 us, 0.12% latency, 14.53 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 304.46 us, 0.12% latency, 14.77 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 71.76 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.82 ms, 0.71% latency, 19.93 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 501.87 us, 0.20% latency, 24.08 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 546.22 us, 0.21% latency, 22.12 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 540.26 us, 0.21% latency, 22.37 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 51.02 us, 0.02% latency, 28.91 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 110.63 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 108.72 us, 0.04% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (25): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 5.11 ms, 2.00% latency, 10.65 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.84 ms, 1.11% latency, 6.39 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 342.85 us, 0.13% latency, 13.11 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 347.14 us, 0.14% latency, 12.95 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 344.04 us, 0.13% latency, 13.07 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 339.27 us, 0.13% latency, 13.25 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.8 ms, 0.70% latency, 20.17 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 538.11 us, 0.21% latency, 22.46 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 492.33 us, 0.19% latency, 24.54 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 544.07 us, 0.21% latency, 22.21 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 51.5 us, 0.02% latency, 28.64 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 128.98 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 130.18 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (26): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.52 ms, 1.77% latency, 12.02 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.26 ms, 0.88% latency, 8.04 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 328.3 us, 0.13% latency, 13.7 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 306.61 us, 0.12% latency, 14.66 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 306.84 us, 0.12% latency, 14.65 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 307.32 us, 0.12% latency, 14.63 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 72.24 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.7 ms, 0.67% latency, 21.37 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 497.58 us, 0.20% latency, 24.29 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 497.58 us, 0.20% latency, 24.29 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 498.29 us, 0.20% latency, 24.25 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 42.92 us, 0.02% latency, 34.37 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 107.77 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 277.76 us, 0.11% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (27): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.8 ms, 1.88% latency, 11.33 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.51 ms, 0.98% latency, 7.23 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 307.56 us, 0.12% latency, 14.62 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 312.33 us, 0.12% latency, 14.4 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 321.63 us, 0.13% latency, 13.98 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 306.61 us, 0.12% latency, 14.66 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.84 ms, 0.72% latency, 19.74 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 534.53 us, 0.21% latency, 22.61 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 527.86 us, 0.21% latency, 22.89 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 541.93 us, 0.21% latency, 22.3 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 50.54 us, 0.02% latency, 29.18 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 110.15 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 147.34 us, 0.06% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (28): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 5.17 ms, 2.03% latency, 10.52 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.69 ms, 1.05% latency, 6.75 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 344.99 us, 0.14% latency, 13.03 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 343.56 us, 0.13% latency, 13.09 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 342.13 us, 0.13% latency, 13.14 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 304.46 us, 0.12% latency, 14.77 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 2.04 ms, 0.80% latency, 17.81 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 523.33 us, 0.21% latency, 23.09 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 527.62 us, 0.21% latency, 22.9 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 545.26 us, 0.21% latency, 22.16 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 247.48 us, 0.10% latency, 5.96 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 134.23 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 109.43 us, 0.04% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (29): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 4.78 ms, 1.87% latency, 11.39 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.43 ms, 0.95% latency, 7.45 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 346.18 us, 0.14% latency, 12.99 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 342.85 us, 0.13% latency, 13.11 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 346.18 us, 0.14% latency, 12.99 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 310.42 us, 0.12% latency, 14.48 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 103.95 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.88 ms, 0.74% latency, 19.26 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 548.12 us, 0.21% latency, 22.05 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 528.81 us, 0.21% latency, 22.85 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 555.52 us, 0.22% latency, 21.75 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 54.84 us, 0.02% latency, 26.9 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 129.7 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 117.54 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (30): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 5.22 ms, 2.05% latency, 10.41 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.84 ms, 1.11% latency, 6.39 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 347.38 us, 0.14% latency, 12.94 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 356.44 us, 0.14% latency, 12.61 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 308.28 us, 0.12% latency, 14.59 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 341.18 us, 0.13% latency, 13.18 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 72.48 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.92 ms, 0.75% latency, 18.91 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 542.16 us, 0.21% latency, 22.29 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 492.33 us, 0.19% latency, 24.54 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 499.25 us, 0.20% latency, 24.2 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 43.39 us, 0.02% latency, 33.99 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 128.27 us, 0.05% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 128.75 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "        (31): LlamaDecoderLayer(\n",
      "          202.38 M, 2.58% Params, 27.19 GMACs, 1.93% MACs, 5.16 ms, 2.02% latency, 10.54 TFLOPS, \n",
      "          (self_attn): LlamaAttention(\n",
      "            67.11 M, 0.86% Params, 9.07 GMACs, 0.64% MACs, 2.85 ms, 1.12% latency, 6.36 TFLOPS, \n",
      "            (q_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 320.2 us, 0.13% latency, 14.04 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 354.53 us, 0.14% latency, 12.68 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 345.23 us, 0.14% latency, 13.02 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(16.78 M, 0.21% Params, 2.25 GMACs, 0.16% MACs, 340.7 us, 0.13% latency, 13.2 TFLOPS, in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.03% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            135.27 M, 1.73% Params, 18.13 GMACs, 1.29% MACs, 1.87 ms, 0.73% latency, 19.43 TFLOPS, \n",
      "            (gate_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 544.31 us, 0.21% latency, 22.2 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 545.5 us, 0.21% latency, 22.15 TFLOPS, in_features=11008, out_features=4096, bias=False)\n",
      "            (up_proj): Linear(45.09 M, 0.58% Params, 6.04 GMACs, 0.43% MACs, 536.44 us, 0.21% latency, 22.53 TFLOPS, in_features=4096, out_features=11008, bias=False)\n",
      "            (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 51.74 us, 0.02% latency, 28.51 GFLOPS, )\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 108.96 us, 0.04% latency, 0.0 FLOPS, )\n",
      "          (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 128.03 us, 0.05% latency, 0.0 FLOPS, )\n",
      "        )\n",
      "      )\n",
      "      (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 153.78 us, 0.06% latency, 0.0 FLOPS, )\n",
      "    )\n",
      "    (lm_head): Linear(131.08 M, 1.67% Params, 17.56 GMACs, 1.25% MACs, 1.47 ms, 0.58% latency, 23.88 TFLOPS, in_features=4096, out_features=32001, bias=False)\n",
      "  )\n",
      "  (llama_proj): Linear(3.15 M, 0.04% Params, 201.33 MMACs, 0.01% MACs, 115.39 us, 0.05% latency, 3.49 TFLOPS, in_features=768, out_features=4096, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "q = questions[0]\n",
    "question = q['question']\n",
    "image_id = q['image_id']\n",
    "image = Image.open(f'../datasets/OK-VQA/image/val2014/COCO_val2014_{str(image_id).zfill(12)}.jpg')\n",
    "image = chat.vis_processor(image).unsqueeze(0).to(torch.float16).to('cuda')\n",
    "\n",
    "with get_accelerator().device(0):\n",
    "    with model.maybe_autocast():\n",
    "        flops, macs, params = get_model_profile(\n",
    "            model,\n",
    "            args=({\n",
    "                'image': image,\n",
    "                'text_input': [question]\n",
    "            }, ),\n",
    "            # input_shape=(1, 3, 224, 224),\n",
    "            print_profile=True,\n",
    "            detailed=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
